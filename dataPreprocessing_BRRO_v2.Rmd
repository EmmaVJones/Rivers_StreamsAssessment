---
title: "Preparing Data for Mary"
author: "Emma Jones"
date: "February 5, 2019"
output: html_document
---

Run in R 3.5.1

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(sf)
library(readxl)
library(shiny)
library(mapview)
library(leaflet)
library(miniUI)

source('snapFunctions.R') # snapping functions
source('snapOrganizationFunctions.R') # functions to do stuff with snapping functions
```

This document walks users through the requisite data preprocessing steps for the 2020 IR Rivers and Streams Assessment decision support application. All initial steps are tested on 2018 IR data and thus need to be rerun for 2020 data when those datasets become available.


### Mary Version

This document demonstrates the necessary steps to get (any) regional office up to speed with prerequisite data organization/processing steps to run the 2020 IR. This version works with BRRO data (WCRO and SCRO) to get Mary ready for assessing.

## Input data

#### Conventionals
Bring in Roger's conventionals dataset. Make it a csv immediately to speed up rendering in app. This is the final data pull from Roger for the 2018 IR. As of today, the 2020 final IR pull is not available. **Subsitute 2020 data in when available.**

```{r conventionals2018}
#conventionals <- read_excel('workingDatasets/CONVENTIONALS_20171010.xlsx',sheet = 'CONVENTIONALS')
#conventionals$FDT_DATE_TIME2 <- as.POSIXct(conventionals$FDT_DATE_TIME, format="%m/%d/%y %H:%M")
#write.csv(conventionals, 'workingDatasets/CONVENTIONALS_20171010.csv', row.names=F)

conventionals <- suppressWarnings(suppressMessages(read_csv('workingDatasets/CONVENTIONALS_20171010.csv')))
#glimpse(conventionals)
```

Work with just BRRO for now. Skip step when working up full state data.

```{r BRROconventionals}
conventionals <- filter(conventionals, Deq_Region == 'Blue Ridge') 
```

Now we need to make a dataset of all UNIQUE StationID's for the region of interest to work with.

```{r BRROconventionalsDistinct}

conventionals_D <- distinct(conventionals, FDT_STA_ID, .keep_all = T) %>%
  select(FDT_STA_ID:FDT_SPG_CODE, STA_LV2_CODE:STA_CBP_NAME)# drop data to avoid any confusion
rm(conventionals)
```


#### Last Cycle's Stations Table

This is a draft from Mary right now, but everything should be in there (Stations and ID305B's). Keep determinations for now even though they are draft.

**There are duplicated rows in this dataset that royally F things up. I have filtered them out for now. Not sure if correct.**

```{r stationTable}
stationTable <- read_excel('data/Emma_Stations2018IR_Draft_Dec.xlsx')

stationTable1 <- stationTable %>%
  group_by(STATION_ID) %>%
  mutate(extra= n()) %>%
  select(STATION_ID, extra, everything())

stationTable2 <- distinct(stationTable, STATION_ID, ID305B_1, ID305B_2, ID305B_3, .keep_all = T)%>%
  group_by(STATION_ID) %>%
  mutate(extra= n()) %>%
  select(STATION_ID, extra, everything()) %>% # still extras so just take distinct STATION_ID 
  distinct(STATION_ID,.keep_all = T) %>%
  select(-extra)
rm(stationTable1)
```


#### Assessment Spatial Layer (last cycle)

Since 2018 isn't published yet, working from 2016 final layer. 

```{r Assessment Unit shapefile}
AUs <- st_read('GIS/va_2016_aus_riverine_WGS84.shp') %>%
  st_transform(crs = 102003) # convert to Albers Equal Area just for snapping
```



## Data Organization Step 1: Get AU's from previous cycle for conventionals data.

This step uses last cycle's ID305B columns and joins that information to the conventionals StationID's. This is where all assessments should start. This information **COULD** change if an AU is split through this assessment cycle's assessment process.

```{r join unique StationIDs}

BRRO_Sites <- mutate(conventionals_D, STATION_ID = FDT_STA_ID) %>% # make joining column
  left_join(stationTable2, by='STATION_ID') %>% # join to get ID305B info
  st_as_sf(coords = c("Longitude", "Latitude"),  # make spatial layer while at it
           remove = F, # don't remove these lat/lon cols from df
           crs = 4269) %>% # add projection, needs to be geographic for now bc entering lat/lng, 
  st_transform( st_crs(AUs)) # now change crs to Albers to make snapping work
  
sum(!is.na(BRRO_Sites$ID305B_1))/nrow(BRRO_Sites)*100 # 73% have last cycle AU info
sum(is.na(BRRO_Sites$ID305B_1))/nrow(BRRO_Sites)*100 # 27% don't have last cycle AU info

```

So basics statistics for now, comparing 2018 IR data from Roger to Mary's almost finished 2018 IR Stations table, still missing 27% of stations ID_305B info??? 

```{r BRRO sites without AU info}
BRRO_Sites_noAU <- filter(BRRO_Sites, is.na(ID305B_1))
```

Looking at the BRRO_Sites_noAU dataset we can see the majority of sites are from other regions, but there are still quite a few WCRO stations without AU info. The majority of these are lake stations (STA_LV1_CODE == 'RESERV'). Makes sense, but we will need to fix that eventually.

## Data Organization Step 1.1: Automate AU snapping for StationID's that did not immediately join to last cycle's Station Table 


```{r snapAUs}
source('snapFunctions.R')

# commented out bc dont want to accidentally run twice
#snapList_AU <- snap_Points_to_Feature_List(BRRO_Sites_noAU,'FDT_STA_ID',AUs, seq(10,50,by=10))
snapList_AU <- readRDS('data/allBRRO_snapList_AU.RDS') #prerun results

# sites that snapped to too many segments
tooMany <- snapCheck(snapList_AU[['sf_output']])
length(unique(tooMany$`Point Unique Identifier`)) #13

# perfect sites
sites <- filter(snapList_AU[['sf_output']], !(`Point Unique Identifier` %in% tooMany$`Point Unique Identifier`)) %>%
  st_set_geometry(NULL) %>%
  mutate(FDT_STA_ID=`Point Unique Identifier`)
nrow(sites) #157


#saveRDS(snapList_AU, 'data/allBRRO_snapList_AU.RDS')
#snapList_AU <- readRDS('data/allBRRO_snapList_AU.RDS')
```

So out of 230 stations we tried to snap to AUs, 157 snapped to only one segment (good), 13 snapped to more than one segment (okay), and 60 snapped to no segments (not awesome).


## Data Organization Step 1.2: Fix StationID's that snapped to too many segments

Now use shiny gadget to help user select appropriate AU for each of the sites that snapped to too many segments.

```{r fix too many snaps}
# make spatial forms of each object in WGS 84
tooMany <- snapCheck(snapList_AU[['sf_output']] ) %>%
  st_transform(4326)# project to WGS84 for plotting
siteWithTooMany <- filter(BRRO_Sites_noAU, FDT_STA_ID %in% unique(tooMany$`Point Unique Identifier`)) %>%
  st_transform(4326)# project to WGS84 for plotting

# empty place to put results, save time on processing and dont put in dataframe immediately bc
# looping is already slow enough
StationID <- as.character(nrow(siteWithTooMany))
ID305B <- as.character(nrow(siteWithTooMany))

for (i in 1:nrow(siteWithTooMany)){
  zz <- AUselecter(filter(tooMany, `Point Unique Identifier` %in% siteWithTooMany[i,]), 
                          siteWithTooMany[i,], i , nrow(siteWithTooMany)) 
  StationID[i] <- as.character(zz[1][[1]])
  ID305B[i] <- as.character(zz[2][[1]])
}

results  <- data.frame(StationID, ID305B, stringsAsFactors = FALSE)

```

Join back to perfect dataset:

```{r back to perfect}
# Combine sites that snapped to a segement perfectly the first time
results1 <- left_join(results, AUs, by='ID305B') %>%
  mutate(`Buffer Distance` = 'User Selected') %>%
  dplyr::rename(`Point Unique Identifier` = 'StationID') %>%
  dplyr::select(`Point Unique Identifier`, `Buffer Distance`, ID305B, OBJECTID, everything(), -geometry) %>%
  bind_rows(sites) %>% # add perfect sites
  mutate(FDT_STA_ID=`Point Unique Identifier`)

# Make a dataset that didnt have AU's when joined to stationTable but now does have AU info thanks to
# auto snapping or manual choice process
BRRO_Sites_noAU_AU <- filter(BRRO_Sites_noAU, FDT_STA_ID %in% results1$`Point Unique Identifier`) %>%
  st_set_geometry(NULL) %>%
  left_join(results1, by = 'FDT_STA_ID') %>%
  mutate(ID305B_1 = ID305B) %>%
  select(FDT_STA_ID, ID305B_1)



BRRO_Sites_AU <- BRRO_Sites %>% # start with sites regardless of AU's
  left_join(BRRO_Sites_noAU_AU, by = 'FDT_STA_ID') %>%
  mutate(ID305B_1 = ifelse(is.na(ID305B_1.x), as.character(ID305B_1.y), ID305B_1.x)) %>% # replace with snapped if NA
  select(FDT_STA_ID:STATION_ID, ID305B_1,ID305B_2:geometry, -c(ID305B_1.x,ID305B_1.y)) 

BRRO_Sites_needVeryManualAU <- filter(BRRO_Sites_AU, is.na(ID305B_1))
#BRRO_Sites_AU <- filter(BRRO_Sites_AU, !is.na(ID305B_1)) # keep these in for WQS snapping purposes
rm(BRRO_Sites_noAU_AU);rm(tooMany);rm(BRRO_Sites_noAU);rm(sites); rm(results1); rm(siteWithTooMany); rm(zz); rm(results)
```


So now we have a sf object named BRRO_Sites_AU that has the same n rows as original data (filter conventionals for unique sites within given regional office) and most of the AU information noted. I have not filtered out the sites that need handsy AU selection bc I still want to snap to WQS before spitting out to user.


## Get WQS info

Moving on with just sites that have AU information (see note above why) I need to use snapbuffermethod again to join with WQS. 

```{r clean up workspace}
rm(list=setdiff(ls(), c("BRRO_Sites","BRRO_Sites_AU","BRRO_Sites_needVeryManualAU",'conventionals_D',"stationTable","stationTable2",
                        "snap_bufferMethod","snap_Point_to_Feature","snap_Points_to_Feature_List","snapCheck",'AUselecter',
                        'basinNameSwitcher', 'snapAndOrganizeWQS', 'snapCheck','WQSselecter')))
```

Now work through each unique basin in Regional office one at a time by snapping to WQS, making user choose WQS when too many snapped in buffer, and spit out answer with sites that have WQS and without.

```{r snapWQS for whole region}
# function Inputs
AUsnappedSites <- BRRO_Sites_AU # ALL sites as sf object, regardless of AU connection or not
WQSfileLocation <- 'C:/updatedWQS/' # file location of WQS shapefiles
basinName <- basinNameSwitcher("James River Basin") #basinNameSwitcher(unique(AUsnappedSites$Basin)[i])
bufferDistances <- seq(10,80,by=10) # note this went up to 80

regionalOutput <- list()
# Loop through multiple basins
for (i in 1:length(unique(AUsnappedSites$Basin))){
  Regional_Sites_AU_basin <- filter(BRRO_Sites_AU, Basin %in% unique(AUsnappedSites$Basin)[i] )
  WQSsnaps <- snapAndOrganizeWQS(Regional_Sites_AU_basin[1:3,], 'C:/updatedWQS/', 
  #WQSsnaps <- snapAndOrganizeWQS(Regional_Sites_AU_basin, 'C:/updatedWQS/', 
                                 basinNameSwitcher(unique(AUsnappedSites$Basin)[i]),  seq(10,80,by=10))
  regionalOutput[[i]] <- WQSsnaps
}

# smash it all back together
RegionalResults <- do.call("rbind", lapply(regionalOutput, data.frame))

# Sites ready for app
RegionalResults_AU_WQS <- filter(RegionalResults, !is.na(ID305B_1) & !is.na(OBJECTID))
write.csv(RegionalResults_AU_WQS,'RegionalResults_AU_WQS.csv',row.names = F)


# Stations missing either WQS or AU information that needs attention
RegionalResults_missing_AU_WQS <- filter(RegionalResults, is.na(ID305B_1) | is.na(OBJECTID))
write.csv(RegionalResults_missing_AU_WQS,'RegionalResults_missing_AU_WQS.csv',row.names = F) 

```

